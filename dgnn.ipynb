{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/NYSE-Temporal-Graph-Construction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd NYSE-Temporal-Graph-Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from os.path import join as join_path\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import kagglehub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import cache\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torch_geometric.nn import GCN, global_mean_pool, GRUAggregation\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.4)\n"
     ]
    }
   ],
   "source": [
    "dataset_path = kagglehub.dataset_download(\"dgawlik/nyse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fundamentals_df = pd.read_csv(join_path(dataset_path,\"fundamentals.csv\"))\n",
    "df = pd.read_csv(join_path(dataset_path, \"prices.csv\"))\n",
    "prices_split_adjusted_df = pd.read_csv(join_path(dataset_path, \"prices-split-adjusted.csv\"))\n",
    "securities_df = pd.read_csv(join_path(dataset_path, \"securities.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of symbols: 501\n"
     ]
    }
   ],
   "source": [
    "all_symbols = prices_split_adjusted_df['symbol'].unique()\n",
    "\n",
    "@cache\n",
    "def symbol_to_int(symbol: str):\n",
    "    return all_symbols.tolist().index(symbol)\n",
    "\n",
    "@cache\n",
    "def int_to_symbol(idx):\n",
    "    return all_symbols[idx]\n",
    "\n",
    "print(\"Num of symbols:\", len(all_symbols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[datetime.date(2010, 1, 4) datetime.date(2010, 1, 5)\n",
      " datetime.date(2010, 1, 6) ... datetime.date(2016, 12, 28)\n",
      " datetime.date(2016, 12, 29) datetime.date(2016, 12, 30)]\n"
     ]
    }
   ],
   "source": [
    "prices_split_adjusted_df['date'] = pd.to_datetime(prices_split_adjusted_df['date']).dt.date\n",
    "dates = prices_split_adjusted_df['date'].sort_values().unique()\n",
    "\n",
    "\n",
    "def any_to_date(date):\n",
    "    if not isinstance(date, pd._libs.tslibs.timestamps.Timestamp):\n",
    "        date = pd.to_datetime(date).date()\n",
    "    return date\n",
    "\n",
    "@cache\n",
    "def date_to_int(date):\n",
    "    date = any_to_date(date)\n",
    "    return dates.tolist().index(date)\n",
    "\n",
    "@cache\n",
    "def int_to_date(idx):\n",
    "    return dates[idx]\n",
    "\n",
    "print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_dates(target_df, _add_missing_symbols=True):\n",
    "\n",
    "    df = target_df.copy()\n",
    "        \n",
    "    df['date'] = df['date'].apply(date_to_int)\n",
    "    df['symbol'] = df['symbol'].apply(symbol_to_int)\n",
    "\n",
    "\n",
    "    df.sort_values(['date', 'symbol'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    for date, group in df.groupby('date'):\n",
    "        if _add_missing_symbols:\n",
    "            group = add_missing_symbols(date, group)\n",
    "        yield date, group\n",
    "\n",
    "def add_missing_symbols(date, group):\n",
    "    missing_symbols = [s_int for s in all_symbols if (s_int:=symbol_to_int(s)) not in group['symbol'].unique()]\n",
    "    if len(missing_symbols) > 0:\n",
    "        group = pd.concat([group, pd.DataFrame({'date': [date] * len(missing_symbols), 'symbol': missing_symbols})])\n",
    "        group = group.sort_values(['date', 'symbol'])\n",
    "        group = group.fillna(0)\n",
    "\n",
    "    return group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(501, 7)\n",
      "(501, 7)\n",
      "(501, 7)\n"
     ]
    }
   ],
   "source": [
    "for date, batch in batch_dates(prices_split_adjusted_df.sort_values('date').head(1000), _add_missing_symbols=False):\n",
    "    print(add_missing_symbols(date, batch).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embdedder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(Embdedder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.layers = torch.nn.Sequential()\n",
    "        if isinstance(hidden_dims, int):\n",
    "            hidden_dims = [hidden_dims]\n",
    "        elif hidden_dims is None or len(hidden_dims) == 0:\n",
    "            self.hidden_dims = []\n",
    "            self.layers.add_module(\"linear0\", torch.nn.Linear(input_dim, output_dim))\n",
    "            self.layers.add_module(\"sigmoid\", torch.nn.Sigmoid())\n",
    "            return\n",
    "\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dims[0]),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        for i in range(len(hidden_dims)):\n",
    "            if i == len(hidden_dims) - 1:\n",
    "                self.layers.add_module(f\"linear{i}\", torch.nn.Linear(hidden_dims[i], output_dim))\n",
    "                self.layers.add_module(f\"sigmoid\", torch.nn.Sigmoid())\n",
    "            else:\n",
    "                self.layers.add_module(f\"linear{i}\", torch.nn.Linear(hidden_dims[i], hidden_dims[i + 1]))\n",
    "                self.layers.add_module(f\"relu{i}\", torch.nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape, x.type())\n",
    "        return self.layers(x)\n",
    "            \n",
    "\n",
    "class GraphConstructor(nn.Module):\n",
    "    def __init__(self, num_nodes, feature_dim):\n",
    "        super(GraphConstructor, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.feature_dim = feature_dim\n",
    "        \n",
    "        # self.embedder = embedder or Embdedder(feature_dim, [feature_dim * 2], feature_dim * 4)\n",
    "        \n",
    "        self.feature_dim = feature_dim #self.embedder.output_dim\n",
    "\n",
    "        self.query = nn.Linear(self.feature_dim, self.feature_dim)\n",
    "        self.key = nn.Linear(self.feature_dim, self.feature_dim)\n",
    "        self.value = nn.Linear(self.feature_dim, self.num_nodes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [num_nodes, feature_dim]\n",
    "\n",
    "        # embeddings = self.embedder(x) # shape: [num_nodes, feature_dim]\n",
    "\n",
    "        query = self.query(x) # shape: [num_nodes, feature_dim]\n",
    "        key = self.key(x) # shape: [num_nodes, feature_dim]\n",
    "        value = self.value(x) # shape: [num_nodes, feature_dim]\n",
    "\n",
    "        # shape: [num_nodes, num_nodes]\n",
    "        attention = torch.matmul(query, key.T) / self.feature_dim ** 0.5\n",
    "        attention = torch.softmax(attention, dim=1)\n",
    "\n",
    "        # shape: [num_nodes, latent_dim]\n",
    "        edge_weight = torch.matmul(attention, value)\n",
    "        edge_weight = torch.sigmoid(edge_weight)\n",
    "        return edge_weight\n",
    "\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self,  num_nodes, feature_dim, hidden_dim=None):\n",
    "        super(Predictor, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        self.hidden_dim = hidden_dim or self.feature_dim\n",
    "\n",
    "        self.graph_constructor = GraphConstructor(num_nodes, self.feature_dim)\n",
    "\n",
    "        self.threshold = nn.Parameter(torch.tensor(0.5))\n",
    "    \n",
    "        self.gcn = GCN(feature_dim, feature_dim, num_layers=2, out_channels=self.hidden_dim, dropout=0.1)\n",
    "        self.gru = nn.GRUCell(self.hidden_dim, self.hidden_dim)\n",
    "        self.linear = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden_state=None):\n",
    "        # x: [date_batch_size, embedding_dim]\n",
    "        # embeddings = self.embedder(x) # shape: [date_batch_size, latent_dim]\n",
    "\n",
    "        A = self.graph_constructor(x) # adjacency matrix, shape: [date_batch_size, date_batch_size]\n",
    "        # print(\"A\", A)\n",
    "        A = A > self.threshold\n",
    "        \n",
    "        # convert Adjacency matrix to edge index\n",
    "        edge_index = torch.nonzero(A, as_tuple=False).t()\n",
    "\n",
    "        x = self.gcn(x, edge_index)\n",
    "        if hidden_state is None:\n",
    "            hidden_state = torch.zeros(x.shape[0], self.hidden_dim).to(device)\n",
    "        h_next = self.gru(x, hidden_state)\n",
    "        y = self.linear(h_next)\n",
    "        y = torch.tanh(y)\n",
    "\n",
    "        return y, h_next\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([501, 5])\n",
      "torch.Size([501, 5]) torch.FloatTensor\n",
      "y torch.Size([501, 1]) tensor([[-0.1575],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.2224],\n",
      "        [-0.3256],\n",
      "        [-0.2121],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2237],\n",
      "        [-0.2060],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2015],\n",
      "        [-0.2224],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2009],\n",
      "        [-0.2237],\n",
      "        [-0.2298],\n",
      "        [-0.2298],\n",
      "        [-0.2097],\n",
      "        [-0.2298],\n",
      "        [-0.2376],\n",
      "        [-0.2298],\n",
      "        [-0.2298],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.2298],\n",
      "        [-0.2060],\n",
      "        [-0.2290],\n",
      "        [-0.2376],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.2009],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.2097],\n",
      "        [-0.2376],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.2221],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2319],\n",
      "        [-0.1910],\n",
      "        [-0.3256],\n",
      "        [-0.2097],\n",
      "        [-0.2224],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.2060],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.2200],\n",
      "        [-0.2224],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.1864],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2290],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2290],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2290],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2237],\n",
      "        [-0.2376],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2224],\n",
      "        [-0.2319],\n",
      "        [-0.2298],\n",
      "        [-0.2298],\n",
      "        [-0.2298],\n",
      "        [-0.2200],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.1828],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.2009],\n",
      "        [-0.2298],\n",
      "        [-0.2290],\n",
      "        [-0.3256],\n",
      "        [-0.2097],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2376],\n",
      "        [-0.2102],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.2319],\n",
      "        [-0.2224],\n",
      "        [-0.2298],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.2376],\n",
      "        [-0.3256],\n",
      "        [-0.2009],\n",
      "        [-0.2224],\n",
      "        [-0.2216],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2224],\n",
      "        [-0.3256],\n",
      "        [-0.2376],\n",
      "        [-0.2097],\n",
      "        [-0.2319],\n",
      "        [-0.1913],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.1804],\n",
      "        [-0.2298],\n",
      "        [-0.2298],\n",
      "        [-0.2298],\n",
      "        [-0.2009],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2060],\n",
      "        [-0.1992],\n",
      "        [-0.2319],\n",
      "        [-0.2097],\n",
      "        [-0.2298],\n",
      "        [-0.2224],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.2319],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.2298],\n",
      "        [-0.2376],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2060],\n",
      "        [-0.2060],\n",
      "        [-0.2196],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2000],\n",
      "        [-0.2376],\n",
      "        [-0.3256],\n",
      "        [-0.2290],\n",
      "        [-0.2298],\n",
      "        [-0.2319],\n",
      "        [-0.3256],\n",
      "        [-0.2291],\n",
      "        [-0.1989],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2206],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2060],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2200],\n",
      "        [-0.3256],\n",
      "        [-0.2200],\n",
      "        [-0.3256],\n",
      "        [-0.2376],\n",
      "        [-0.1754],\n",
      "        [-0.3256],\n",
      "        [-0.2060],\n",
      "        [-0.2298],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.1804],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.2224],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.2298],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2009],\n",
      "        [-0.3256],\n",
      "        [-0.2224],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.2291],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2224],\n",
      "        [-0.2319],\n",
      "        [-0.2224],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.2298],\n",
      "        [-0.2200],\n",
      "        [-0.2060],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.2060],\n",
      "        [-0.2298],\n",
      "        [-0.2060],\n",
      "        [-0.3256],\n",
      "        [-0.2319],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.2041],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2134],\n",
      "        [-0.3256],\n",
      "        [-0.2051],\n",
      "        [-0.2376],\n",
      "        [-0.2224],\n",
      "        [-0.1910],\n",
      "        [-0.2298],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.2290],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.2376],\n",
      "        [-0.2060],\n",
      "        [-0.2290],\n",
      "        [-0.3256],\n",
      "        [-0.2060],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2376],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2128],\n",
      "        [-0.3256],\n",
      "        [-0.2311],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2216],\n",
      "        [-0.3256],\n",
      "        [-0.2290],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2319],\n",
      "        [-0.1763],\n",
      "        [-0.3256],\n",
      "        [-0.2319],\n",
      "        [-0.2009],\n",
      "        [-0.3256],\n",
      "        [-0.2221],\n",
      "        [-0.2376],\n",
      "        [-0.2009],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.2298],\n",
      "        [-0.2319],\n",
      "        [-0.2298],\n",
      "        [-0.2319],\n",
      "        [-0.2290],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.2298],\n",
      "        [-0.2298],\n",
      "        [-0.2191],\n",
      "        [-0.3256],\n",
      "        [-0.2060],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2319],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2206],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.2376],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2200],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2237],\n",
      "        [-0.2298],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.2009],\n",
      "        [-0.2290],\n",
      "        [-0.1965],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.2097],\n",
      "        [-0.2200],\n",
      "        [-0.3256],\n",
      "        [-0.2224],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.2089],\n",
      "        [-0.2298],\n",
      "        [-0.1973],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2051],\n",
      "        [-0.2298],\n",
      "        [-0.2097],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2200],\n",
      "        [-0.2298],\n",
      "        [-0.1973],\n",
      "        [-0.2015],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2376],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.2060],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.2060],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.1992],\n",
      "        [-0.2298],\n",
      "        [-0.2298],\n",
      "        [-0.2097],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2009],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.1777],\n",
      "        [-0.3256],\n",
      "        [-0.2298],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.2009],\n",
      "        [-0.1804],\n",
      "        [-0.1437],\n",
      "        [-0.1674],\n",
      "        [-0.3256],\n",
      "        [-0.1674],\n",
      "        [-0.1665],\n",
      "        [-0.1674],\n",
      "        [-0.3256],\n",
      "        [-0.1678],\n",
      "        [-0.3256],\n",
      "        [-0.1523],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.1623],\n",
      "        [-0.3256],\n",
      "        [-0.1678],\n",
      "        [-0.3256],\n",
      "        [-0.1674],\n",
      "        [-0.1575],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.1674],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.1623],\n",
      "        [-0.1437],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.3256],\n",
      "        [-0.1674],\n",
      "        [-0.3256],\n",
      "        [-0.1674],\n",
      "        [-0.1678]], grad_fn=<TanhBackward0>)\n",
      "hidden_state torch.Size([501, 20])\n",
      "14743\n"
     ]
    }
   ],
   "source": [
    "d = None\n",
    "for _, batch in batch_dates(prices_split_adjusted_df.sort_values('date').head(1000)):\n",
    "    d = batch\n",
    "    break\n",
    "\n",
    "num_nodes = len(all_symbols)\n",
    "emb_dim = 5 * 4\n",
    "embedder = Embdedder(5, [5 * 2], emb_dim)\n",
    "model = Predictor(num_nodes, emb_dim).to(device)\n",
    "\n",
    "d = d.drop(columns=['date', 'symbol']).to_numpy()\n",
    "\n",
    "d = torch.tensor(d).to(device, dtype=torch.float32)\n",
    "\n",
    "print(d.shape)\n",
    "y, hidden_state = model(embedder(d))\n",
    "print(\"y\", y.shape, y)\n",
    "print(\"hidden_state\", hidden_state.shape)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(total_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(predictor, train_loader, optimizer, device):\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    \n",
    "    predictor.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = predictor(data)\n",
    "        loss = criterion(output, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
